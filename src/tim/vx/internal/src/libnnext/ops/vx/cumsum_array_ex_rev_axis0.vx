#include "cl_viv_vx_ext.h"

_viv_uniform VXC_512Bits uniConvertInt32toUint8_2x8;

_viv_uniform VXC_512Bits uniSumHorzF16toF16A_4x4;
_viv_uniform VXC_512Bits uniSumHorzF16toF16B_4x4;
_viv_uniform VXC_512Bits uniSumHorzF16toF16C_2x8;
_viv_uniform VXC_512Bits uniAccSumHorzF16toF16_2x8;
_viv_uniform VXC_512Bits uniSumHorzU8toI16A_4x4;
_viv_uniform VXC_512Bits uniSumHorzU8toI16B_8x4;
_viv_uniform VXC_512Bits uniSubZpI16toI16_2x8;
_viv_uniform VXC_512Bits uniAccSumHorzI16toI32A_4x4;
_viv_uniform VXC_512Bits uniAccSumHorzI16toI32B_4x4;

_viv_uniform VXC_512Bits uniSetZeroF16_2x8;

_viv_uniform VXC_512Bits uniSumHorzRevF16toF16A_4x4;
_viv_uniform VXC_512Bits uniSumHorzRevF16toF16B_4x4;
_viv_uniform VXC_512Bits uniSumHorzRevF16toF16C_2x8;
_viv_uniform VXC_512Bits uniAccSumHorzRevF16toF16_2x8;

_viv_uniform VXC_512Bits uniSumHorzRevU8toI16A_4x4;
_viv_uniform VXC_512Bits uniSumHorzRevU8toI16B_8x4;
_viv_uniform VXC_512Bits uniSubZpRevI16toI16_2x8;
_viv_uniform VXC_512Bits uniAccSumHorzRevI16toI32A_4x4;
_viv_uniform VXC_512Bits uniAccSumHorzRevI16toI32B_4x4;


_viv_uniform int width;
_viv_uniform int input_zp;
_viv_uniform float in_out_scale;
_viv_uniform float output_zp;

_viv_uniform int remainder;
_viv_uniform int w_size;


__kernel void cumsum_ex_rev_array_F16toF16_axis0(
    __read_only image2d_array_t   input,
    __write_only image2d_array_t  output,
    int axis, int exclusive, int rev
    )
{
    int4 coord = (int4)(0, get_global_id(1), get_global_id(2), 0);
    int4 coord_out = coord;

    vxc_short8 src, dst;
    vxc_half8 data, tmpsum, sum;
    VXC_DP2x8(sum, sum, sum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniSetZeroF16_2x8);

    Tensor img1 = create_tensor_from_image2d_array(input, 2);
    Tensor img2 = create_tensor_from_image2d_array(output, 2);
    uchar* input_ptr = get_tensor_ptr_from_coord(img1, coord);
    uchar* output_ptr = get_tensor_ptr_from_coord(img2, coord);
    __global vxc_short8* in_ptr = (__global vxc_short8*)input_ptr;
    __global vxc_short8* out_ptr = (__global vxc_short8*)output_ptr;
    if(exclusive == 0 && rev)
    {
        for(coord.x = width - 8; coord.x >= 0; coord.x -= 8)
        {
            if (coord.x == ((w_size >> 3) * 8) && remainder != 0)
            {
                coord.x = coord.x - (8 - remainder);
            }
            input_ptr = get_tensor_ptr_from_coord(img1, coord);
            output_ptr = get_tensor_ptr_from_coord(img2, coord);
            in_ptr = (__global vxc_short8*)input_ptr;
            out_ptr = (__global vxc_short8*)output_ptr;
            src = in_ptr[0];
            _viv_asm(COPY, data, src, 16);

            VXC_DP4x4(tmpsum, data, data, VXC_MODIFIER(4, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzRevF16toF16A_4x4);
            VXC_DP4x4(tmpsum, data, data, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), uniSumHorzRevF16toF16B_4x4);
            VXC_DP2x8(tmpsum, tmpsum, tmpsum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0),
                        uniSumHorzRevF16toF16C_2x8);
            VXC_DP2x8(sum, tmpsum, sum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniAccSumHorzRevF16toF16_2x8);
            _viv_asm(COPY, dst, sum, 16);
            out_ptr[0] = dst;
        }
    }
    else if(exclusive && rev == 0)
    {
        _viv_asm(COPY, dst, sum, 16);
        out_ptr[0] = dst;
        for(; coord.x < width - 8;)
        {
            if (coord.x == ((w_size >> 3) * 8) && remainder != 0)
            {
                coord.x = coord.x - (8 - remainder);
            }
            input_ptr = get_tensor_ptr_from_coord(img1, coord);
            in_ptr = (__global vxc_short8*)input_ptr;
            src = in_ptr[0];
            coord_out.x = coord.x + 1;
            coord.x += 8;
            _viv_asm(COPY, data, src, 16);

            output_ptr = get_tensor_ptr_from_coord(img2, coord_out);
            out_ptr = (__global vxc_short8*)output_ptr;
            VXC_DP4x4(tmpsum, data, data, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), uniSumHorzF16toF16A_4x4);
            VXC_DP4x4(tmpsum, data, data, VXC_MODIFIER(4, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzF16toF16B_4x4);
            VXC_DP2x8(tmpsum, tmpsum, tmpsum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzF16toF16C_2x8);
            VXC_DP2x8(sum, tmpsum, sum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniAccSumHorzF16toF16_2x8);
            _viv_asm(COPY, dst, sum, 16);
            out_ptr[0] = dst;
        }
    }
    else if(exclusive && rev)
    {
        coord.x = width - 8;
        coord_out.x = width - 1;
        _viv_asm(COPY, dst, sum, 16);
        output_ptr = get_tensor_ptr_from_coord(img2, coord_out);
        out_ptr = (__global vxc_short8*)output_ptr;
        out_ptr[0] = dst;
        for(; coord.x > 0;)
        {
            if (coord.x == ((w_size >> 3) * 8) && remainder != 0)
            {
                coord.x = coord.x - (8 - remainder);
            }
            input_ptr = get_tensor_ptr_from_coord(img1, coord);
            output_ptr = get_tensor_ptr_from_coord(img2, coord);
            in_ptr = (__global vxc_short8*)input_ptr;
            out_ptr = (__global vxc_short8*)output_ptr;
            src = in_ptr[0];
            coord_out.x = coord.x - 1;
            coord.x -= 8;
            _viv_asm(COPY, data, src, 16);

            VXC_DP4x4(tmpsum, data, data, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), uniSumHorzRevF16toF16A_4x4);
            VXC_DP4x4(tmpsum, data, data, VXC_MODIFIER(4, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzRevF16toF16B_4x4);
            VXC_DP2x8(tmpsum, tmpsum, tmpsum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0),
                        uniSumHorzRevF16toF16C_2x8);
            VXC_DP2x8(sum, tmpsum, sum, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniAccSumHorzRevF16toF16_2x8);
            _viv_asm(COPY, dst, sum, 16);
            out_ptr[0] = dst;
        }
    }
}

#define CUMSUM_QINT_EX_REV_ARRAY_AXIS0(in_name, out_name, src_type, dst_type, stride_data) \
__kernel void cumsum_ex_rev_array_##in_name##to##out_name##_axis0( \
    __read_only image2d_array_t   input, \
    __write_only image2d_array_t  output, \
    int axis, int exclusive, int rev \
    ) \
{ \
    int4 coord = (int4)(0, get_global_id(1), get_global_id(2), 0); \
    int4 coord_out = coord; \
 \
    src_type src; \
    dst_type dst; \
    vxc_short8 rowSum; \
    int4 sum0 = (int4)(0), sum1 = (int4)(0); \
    short zp = (short)input_zp; \
 \
    Tensor img1 = create_tensor_from_image2d_array(input, stride_data); \
    Tensor img2 = create_tensor_from_image2d_array(output, stride_data); \
    uchar* input_ptr = get_tensor_ptr_from_coord(img1, coord); \
    uchar* output_ptr = get_tensor_ptr_from_coord(img2, coord); \
    __global src_type* in_ptr = (__global src_type*)input_ptr; \
    __global dst_type* out_ptr = (__global dst_type*)output_ptr; \
    if(exclusive == 0 && rev) \
    { \
        for(coord.x = width - 8; coord.x >= 0; coord.x -= 8) \
        { \
            if (coord.x == ((w_size >> 3) * 8) && remainder != 0) \
            { \
                coord.x = coord.x - (8 - remainder); \
            } \
            input_ptr = get_tensor_ptr_from_coord(img1, coord); \
            output_ptr = get_tensor_ptr_from_coord(img2, coord); \
            in_ptr = (__global src_type*)input_ptr; \
            out_ptr = (__global dst_type*)output_ptr; \
            src = in_ptr[0]; \
            VXC_ReadImage2DArray(src, input, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
            VXC_DP4x4(rowSum, src, src, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), uniSumHorzRevU8toI16A_4x4); \
            VXC_DP8x4(rowSum, src, src, VXC_MODIFIER(4, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzRevU8toI16B_8x4); \
            VXC_DP2x8(rowSum, rowSum, zp, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniSubZpRevI16toI16_2x8); \
            VXC_DP4x4(sum0, rowSum, sum1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
                        uniAccSumHorzRevI16toI32A_4x4); \
            VXC_DP4x4(sum1, rowSum, sum1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
                        uniAccSumHorzRevI16toI32B_4x4); \
            float4 tmpSum0 = convert_float4(sum0) * in_out_scale + output_zp; \
            float4 tmpSum1 = convert_float4(sum1) * in_out_scale + output_zp; \
            int4 tmpDst0 = convert_int4_rte(tmpSum0); \
            int4 tmpDst1 = convert_int4_rte(tmpSum1); \
            VXC_DP2x8(dst, tmpDst1, tmpDst0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
                        uniConvertInt32toUint8_2x8); \
            out_ptr[0] = dst; \
        } \
    } \
    else if(exclusive && rev == 0) \
    { \
        for(coord.x = -1; coord.x < width - 8;) \
        { \
            if (coord.x == ((w_size >> 3) * 8) && remainder != 0) \
            { \
                coord.x = coord.x - (8 - remainder); \
            } \
            input_ptr = get_tensor_ptr_from_coord(img1, coord); \
            in_ptr = (__global src_type*)input_ptr; \
            src = in_ptr[0]; \
            coord_out.x = coord.x + 1; \
            coord.x += 8; \
            output_ptr = get_tensor_ptr_from_coord(img2, coord_out); \
            out_ptr = (__global dst_type*)output_ptr; \
            VXC_DP4x4(rowSum, src, src, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), uniSumHorzU8toI16A_4x4); \
            VXC_DP8x4(rowSum, src, src, VXC_MODIFIER(4, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzU8toI16B_8x4); \
            VXC_DP2x8(rowSum, rowSum, zp, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniSubZpI16toI16_2x8); \
            VXC_DP4x4(sum0, rowSum, sum1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
                        uniAccSumHorzI16toI32A_4x4); \
            VXC_DP4x4(sum1, rowSum, sum1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
                        uniAccSumHorzI16toI32B_4x4); \
            float4 tmpSum0 = convert_float4(sum0) * in_out_scale + output_zp; \
            float4 tmpSum1 = convert_float4(sum1) * in_out_scale + output_zp; \
            int4 tmpDst0 = convert_int4_rte(tmpSum0); \
            int4 tmpDst1 = convert_int4_rte(tmpSum1); \
            VXC_DP2x8(dst, tmpDst0, tmpDst1, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
                        uniConvertInt32toUint8_2x8); \
            out_ptr[0] = dst; \
        } \
    } \
    else if(exclusive && rev) \
    { \
        for(coord.x = width - 7; coord.x > 0;) \
        { \
            if (coord.x == ((w_size >> 3) * 8) && remainder != 0) \
            { \
                coord.x = coord.x - (8 - remainder); \
            } \
            input_ptr = get_tensor_ptr_from_coord(img1, coord); \
            output_ptr = get_tensor_ptr_from_coord(img2, coord); \
            in_ptr = (__global src_type*)input_ptr; \
            out_ptr = (__global dst_type*)output_ptr; \
            src = in_ptr[0]; \
            coord_out.x = coord.x - 1; \
            coord.x -= 8; \
            VXC_DP4x4(rowSum, src, src, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), uniSumHorzRevU8toI16A_4x4); \
            VXC_DP8x4(rowSum, src, src, VXC_MODIFIER(4, 7, 0, VXC_RM_TowardZero, 0), uniSumHorzRevU8toI16B_8x4); \
            VXC_DP2x8(rowSum, rowSum, zp, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0), uniSubZpRevI16toI16_2x8); \
            VXC_DP4x4(sum0, rowSum, sum1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
                        uniAccSumHorzRevI16toI32A_4x4); \
            VXC_DP4x4(sum1, rowSum, sum1, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
                        uniAccSumHorzRevI16toI32B_4x4); \
            float4 tmpSum0 = convert_float4(sum0) * in_out_scale + output_zp; \
            float4 tmpSum1 = convert_float4(sum1) * in_out_scale + output_zp; \
            int4 tmpDst0 = convert_int4_rte(tmpSum0); \
            int4 tmpDst1 = convert_int4_rte(tmpSum1); \
            VXC_DP2x8(dst, tmpDst1, tmpDst0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
                        uniConvertInt32toUint8_2x8); \
            out_ptr[0] = dst; \
        } \
    } \
}
CUMSUM_QINT_EX_REV_ARRAY_AXIS0(U8,  U8,  vxc_uchar16, vxc_uchar16, 1)
CUMSUM_QINT_EX_REV_ARRAY_AXIS0(I8,  I8,  vxc_char16,  vxc_char16, 1)
CUMSUM_QINT_EX_REV_ARRAY_AXIS0(I16, I16, vxc_short8,  vxc_short8, 2)
