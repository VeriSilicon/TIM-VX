#include "cl_viv_vx_ext.h"

_viv_uniform VXC_512Bits uniSum_X_X2_8x2;
_viv_uniform VXC_512Bits uniDataToFP32_0_4x4;
_viv_uniform VXC_512Bits uniDataToFP32_1_4x4;
_viv_uniform VXC_512Bits uniExtract8Data_2x8;
_viv_uniform int width;
_viv_uniform float inv_multiplier;
_viv_uniform float output_scale;
_viv_uniform float output_zp;

#define CONV2F32(dst, src, section) \
        VXC_DP4x4(dst, src, src, VXC_MODIFIER(0, 3, 0, VXC_RM_TowardZero, 0), \
            uniDataToFP32_##section##_4x4);

#define LAYER_NORM_16BITS_IMPL(name, src_type, dst_type, copy_type, conv_type) \
__kernel void layer_norm_axis0_##name( \
    __read_only  image2d_array_t input, \
    __read_only  image2d_t       bias, \
    __read_only  image2d_t       scale, \
    __write_only image2d_array_t output, \
                 float           eps) \
{ \
    int4 coord = (int4)(0, get_global_id(1), get_global_id(2), get_global_id(2)); \
    int4 coord_out = coord; \
 \
    vxc_short8 in0; \
    src_type src0; \
    copy_type dst; \
    vxc_short8 src1; \
    dst_type result; \
    vxc_half8 scale_h; \
    float4 bias_f0, bias_f1, scale_f0, scale_f1; \
    float2 _sums = 0, sum_x_x2; \
 \
    int8 input_desc, output_desc; \
    _viv_asm(COPY, input_desc, input, sizeof(input_desc)); \
    int baseAddr_a = (int)get_global_id(2) * input_desc.s4 + input_desc.s0; \
    _viv_asm(MOV, coord.z, baseAddr_a); \
 \
    _viv_asm(COPY, output_desc, output, sizeof(output_desc)); \
    int baseAddr = (int)get_global_id(2) * output_desc.s4 + output_desc.s0; \
    _viv_asm(MOV, coord_out.z, baseAddr); \
 \
    for (coord.x = 0; coord.x < width; ) \
    { \
        VXC_OP4(img_load_3d, in0, input, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        coord.x += 8; \
        VXC_DP8x2(sum_x_x2, src0, src0, VXC_MODIFIER(0, 1, 0, VXC_RM_TowardZero, 0), uniSum_X_X2_8x2); \
        _sums = _sums + sum_x_x2; \
    } \
 \
    float2 sums = _sums * inv_multiplier; \
 \
    sums.y = sums.y - sums.x * sums.x + eps; \
    sums.y = rsqrt(sums.y); \
    conv_type tmpVal0, tmpVal1; \
    float4  tmpData0, tmpData1; \
    int2 coord_bias = (int2)(0, 0); \
 \
    for(coord.x = 0; coord.x < width; coord.x += 8) \
    { \
        VXC_OP4(img_load_3d, in0, input, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        VXC_ReadImage(src1, scale, coord.xw, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        coord_bias.x = coord.x; \
        _viv_asm(COPY, scale_h, src1, 16); \
        CONV2F32(scale_f0, scale_h, 0); \
        CONV2F32(scale_f1, scale_h, 1); \
        bias_f0 = read_imagef(bias, coord_bias); \
        coord_bias.x += 4; \
        bias_f1 = read_imagef(bias, coord_bias); \
        coord_bias.x += 4; \
 \
        CONV2F32(tmpData0, src0, 0); \
        CONV2F32(tmpData1, src0, 1); \
 \
        float4 norm; \
        tmpData0 = tmpData0 - sums.x; \
        norm = scale_f0 * sums.y * tmpData0 + bias_f0; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal0, norm); \
 \
        tmpData1 = tmpData1 - sums.x; \
        norm = scale_f1 * sums.y * tmpData1 + bias_f1; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal1, norm); \
        VXC_DP2x8(result, tmpVal0, tmpVal1, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
            uniExtract8Data_2x8); \
        _viv_asm(COPY, dst, result, 16); \
 \
        coord_out.x = coord.x; \
        VXC_OP4_NoDest(img_store_3d, output, coord_out, dst, VXC_MODIFIER(0, 7, 0,VXC_RM_TowardZero, 0)); \
    } \
}
LAYER_NORM_16BITS_IMPL(F16_F16toF16, vxc_half8,  vxc_half8,  vxc_short8, half4)
LAYER_NORM_16BITS_IMPL(F16_F16toI16, vxc_half8,  vxc_short8, vxc_short8, int4)
LAYER_NORM_16BITS_IMPL(F16_F16toI8,  vxc_half8,  vxc_char8,  vxc_char8,  int4)
LAYER_NORM_16BITS_IMPL(F16_F16toU8,  vxc_half8,  vxc_uchar8, vxc_uchar8, int4)
LAYER_NORM_16BITS_IMPL(I16_F16toI16, vxc_short8, vxc_short8, vxc_short8, int4)
LAYER_NORM_16BITS_IMPL(I16_F16toF16, vxc_short8, vxc_half8,  vxc_short8, half4)

#define LAYER_NORM_16BITS_IMPL_2D(name, src_type, dst_type, copy_type, conv_type) \
__kernel void layer_norm_axis0_##name##_2D( \
    __read_only  image2d_array_t input, \
    __read_only  image2d_t       bias, \
    __read_only  image2d_t       scale, \
    __write_only image2d_array_t output, \
                 float           eps) \
{ \
    int4 coord = (int4)(0, get_global_id(1), 0, 0); \
 \
    vxc_short8 in0; \
    src_type src0; \
    copy_type dst; \
    dst_type result; \
    vxc_short8 src1; \
    vxc_half8 scale_h; \
    float4 bias_f0, bias_f1, scale_f0, scale_f1; \
    float2 _sums = 0, sum_x_x2; \
 \
    for (coord.x = 0; coord.x < width; ) \
    { \
        VXC_ReadImage(in0, input, coord.xy, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        coord.x += 8; \
        VXC_DP8x2(sum_x_x2, src0, src0, VXC_MODIFIER(0, 1, 0, VXC_RM_TowardZero, 0), uniSum_X_X2_8x2); \
        _sums = _sums + sum_x_x2; \
    } \
 \
    float2 sums = _sums * inv_multiplier; \
 \
    sums.y = sums.y - sums.x * sums.x + eps; \
    sums.y = rsqrt(sums.y); \
 \
    conv_type tmpVal0, tmpVal1; \
    float4  tmpData0, tmpData1; \
    int2 coord_bias = (int2)(0, 0); \
 \
    for (coord.x = 0; coord.x < width; coord.x += 8) \
    { \
        VXC_ReadImage(in0, input, coord.xy, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        VXC_ReadImage(src1, scale, coord.xw, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        coord_bias.x = coord.x; \
        _viv_asm(COPY, scale_h, src1, 16); \
        CONV2F32(scale_f0, scale_h, 0); \
        CONV2F32(scale_f1, scale_h, 1); \
        bias_f0 = read_imagef(bias, coord_bias); \
        coord_bias.x += 4; \
        bias_f1 = read_imagef(bias, coord_bias); \
        coord_bias.x += 4; \
 \
        CONV2F32(tmpData0, src0, 0); \
        CONV2F32(tmpData1, src0, 1); \
 \
        float4 norm; \
        tmpData0 = tmpData0 - sums.x; \
        norm = scale_f0 * sums.y * tmpData0 + bias_f0; \
        coord_bias.x += 4; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal0, norm); \
 \
        tmpData1 = tmpData1 - sums.x; \
        norm = scale_f1 * sums.y * tmpData1 + bias_f1; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal1, norm); \
        VXC_DP2x8(result, tmpVal0, tmpVal1, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
            uniExtract8Data_2x8); \
        _viv_asm(COPY, dst, result, 16); \
 \
        VXC_WriteImage(output, coord.xy, dst, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
    } \
}
LAYER_NORM_16BITS_IMPL_2D(F16_F16toF16, vxc_half8,  vxc_half8,  vxc_short8, half4)
LAYER_NORM_16BITS_IMPL_2D(F16_F16toI16, vxc_half8,  vxc_short8, vxc_short8, int4)
LAYER_NORM_16BITS_IMPL_2D(F16_F16toI8,  vxc_half8,  vxc_char8,  vxc_char8,  int4)
LAYER_NORM_16BITS_IMPL_2D(F16_F16toU8,  vxc_half8,  vxc_uchar8, vxc_uchar8, int4)
LAYER_NORM_16BITS_IMPL_2D(I16_F16toI16, vxc_short8, vxc_short8, vxc_short8, int4)
LAYER_NORM_16BITS_IMPL_2D(I16_F16toF16, vxc_short8, vxc_half8,  vxc_short8, half4)

#define LAYER_NORM_16_32_IMPL(name, src_type, dst_type, copy_type, conv_type) \
__kernel void layer_norm_axis0_##name( \
    __read_only  image2d_array_t input, \
    __read_only  image2d_t       bias, \
    __read_only  image2d_t       scale, \
    __write_only image2d_array_t output, \
                 float           eps) \
{ \
    int4 coord = (int4)(0, get_global_id(1), get_global_id(2), get_global_id(2)); \
    int4 coord_out = coord; \
 \
    vxc_short8 in0; \
    src_type src0; \
    copy_type dst; \
    dst_type result; \
    float4 bias_f0, bias_f1, scale_f0, scale_f1; \
    float2 _sums = 0, sum_x_x2; \
 \
    int8 input_desc, output_desc; \
    _viv_asm(COPY, input_desc, input, sizeof(input_desc)); \
    int baseAddr_a = (int)get_global_id(2) * input_desc.s4 + input_desc.s0; \
    _viv_asm(MOV, coord.z, baseAddr_a); \
 \
    _viv_asm(COPY, output_desc, output, sizeof(output_desc)); \
    int baseAddr = (int)get_global_id(2) * output_desc.s4 + output_desc.s0; \
    _viv_asm(MOV, coord_out.z, baseAddr); \
 \
    for (coord.x = 0; coord.x < width; ) \
    { \
        VXC_OP4(img_load_3d, in0, input, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        coord.x += 8; \
        VXC_DP8x2(sum_x_x2, src0, src0, VXC_MODIFIER(0, 1, 0, VXC_RM_TowardZero, 0), uniSum_X_X2_8x2); \
        _sums = _sums + sum_x_x2; \
    } \
 \
    float2 sums = _sums * inv_multiplier; \
 \
    sums.y = sums.y - sums.x * sums.x + eps; \
    sums.y = rsqrt(sums.y); \
    conv_type tmpVal0, tmpVal1; \
    float4  tmpData0, tmpData1; \
 \
    Image img1 = create_image_from_image2d(bias, 4); \
    Image img2 = create_image_from_image2d(scale, 4); \
    __global float* bias_ptr = (__global float*)img1.ptr; \
    __global float* scale_ptr = (__global float*)img2.ptr; \
    for(coord.x = 0; coord.x < width; coord.x += 8) \
    { \
        VXC_OP4(img_load_3d, in0, input, coord, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        bias_f0 = vload4(0, bias_ptr); \
        bias_f1 = vload4(1, bias_ptr); \
        scale_f0 = vload4(0, scale_ptr); \
        scale_f1 = vload4(1, scale_ptr); \
        bias_ptr += 8; \
        scale_ptr += 8; \
 \
        CONV2F32(tmpData0, src0, 0); \
        CONV2F32(tmpData1, src0, 1); \
 \
        float4 norm; \
        tmpData0 = tmpData0 - sums.x; \
        norm = scale_f0 * sums.y * tmpData0 + bias_f0; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal0, norm); \
 \
        tmpData1 = tmpData1 - sums.x; \
        norm = scale_f1 * sums.y * tmpData1 + bias_f1; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal1, norm); \
        VXC_DP2x8(result, tmpVal0, tmpVal1, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
            uniExtract8Data_2x8); \
        _viv_asm(COPY, dst, result, 16); \
 \
        coord_out.x = coord.x; \
        VXC_OP4_NoDest(img_store_3d, output, coord_out, dst, VXC_MODIFIER(0, 7, 0,VXC_RM_TowardZero, 0)); \
    } \
}
LAYER_NORM_16_32_IMPL(F16_F32toF16, vxc_half8,  vxc_half8,  vxc_short8, half4)
LAYER_NORM_16_32_IMPL(F16_F32toI16, vxc_half8,  vxc_short8, vxc_short8, int4)
LAYER_NORM_16_32_IMPL(F16_F32toI8,  vxc_half8,  vxc_char8,  vxc_char8,  int4)
LAYER_NORM_16_32_IMPL(F16_F32toU8,  vxc_half8,  vxc_uchar8, vxc_uchar8, int4)
LAYER_NORM_16_32_IMPL(I16_F32toI16, vxc_short8, vxc_short8, vxc_short8, int4)
LAYER_NORM_16_32_IMPL(I16_F32toF16, vxc_short8, vxc_half8,  vxc_short8, half4)

#define LAYER_NORM_16_32_IMPL_2D(name, src_type, dst_type, copy_type, conv_type) \
__kernel void layer_norm_axis0_##name##_2D( \
    __read_only  image2d_array_t input, \
    __read_only  image2d_t       bias, \
    __read_only  image2d_t       scale, \
    __write_only image2d_array_t output, \
                 float           eps) \
{ \
    int4 coord = (int4)(0, get_global_id(1), 0, 0); \
 \
    vxc_short8 in0; \
    src_type src0; \
    copy_type dst; \
    dst_type result; \
    float4 bias_f0, bias_f1, scale_f0, scale_f1; \
    float2 _sums = 0, sum_x_x2; \
 \
    for (coord.x = 0; coord.x < width; ) \
    { \
        VXC_ReadImage(in0, input, coord.xy, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        coord.x += 8; \
        VXC_DP8x2(sum_x_x2, src0, src0, VXC_MODIFIER(0, 1, 0, VXC_RM_TowardZero, 0), uniSum_X_X2_8x2); \
        _sums = _sums + sum_x_x2; \
    } \
 \
    float2 sums = _sums * inv_multiplier; \
 \
    sums.y = sums.y - sums.x * sums.x + eps; \
    sums.y = rsqrt(sums.y); \
 \
    conv_type tmpVal0, tmpVal1; \
    float4  tmpData0, tmpData1; \
 \
    Image img1 = create_image_from_image2d(bias, 4); \
    Image img2 = create_image_from_image2d(scale, 4); \
    __global float* bias_ptr = (__global float*)img1.ptr; \
    __global float* scale_ptr = (__global float*)img2.ptr; \
    for (coord.x = 0; coord.x < width; coord.x += 8) \
    { \
        VXC_ReadImage(in0, input, coord.xy, 0, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
        _viv_asm(COPY, src0, in0, 16); \
        bias_f0 = vload4(0, bias_ptr); \
        bias_f1 = vload4(1, bias_ptr); \
        scale_f0 = vload4(0, scale_ptr); \
        scale_f1 = vload4(1, scale_ptr); \
        bias_ptr += 8; \
        scale_ptr += 8; \
 \
        CONV2F32(tmpData0, src0, 0); \
        CONV2F32(tmpData1, src0, 1); \
 \
        float4 norm; \
        tmpData0 = tmpData0 - sums.x; \
        norm = scale_f0 * sums.y * tmpData0 + bias_f0; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal0, norm); \
 \
        tmpData1 = tmpData1 - sums.x; \
        norm = scale_f1 * sums.y * tmpData1 + bias_f1; \
        norm = norm * output_scale + output_zp; \
        _viv_asm(CONV_RTE, tmpVal1, norm); \
        VXC_DP2x8(result, tmpVal0, tmpVal1, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 1), \
            uniExtract8Data_2x8); \
        _viv_asm(COPY, dst, result, 16); \
 \
        VXC_WriteImage(output, coord.xy, dst, VXC_MODIFIER(0, 7, 0, VXC_RM_TowardZero, 0)); \
    } \
}
LAYER_NORM_16_32_IMPL_2D(F16_F32toF16, vxc_half8,  vxc_half8,  vxc_short8, half4)
LAYER_NORM_16_32_IMPL_2D(F16_F32toI16, vxc_half8,  vxc_short8, vxc_short8, int4)
LAYER_NORM_16_32_IMPL_2D(F16_F32toI8,  vxc_half8,  vxc_char8,  vxc_char8,  int4)
LAYER_NORM_16_32_IMPL_2D(F16_F32toU8,  vxc_half8,  vxc_uchar8, vxc_uchar8, int4)
LAYER_NORM_16_32_IMPL_2D(I16_F32toI16, vxc_short8, vxc_short8, vxc_short8, int4)
LAYER_NORM_16_32_IMPL_2D(I16_F32toF16, vxc_short8, vxc_half8,  vxc_short8, half4)
